{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 22:26:14.891108: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-06 22:26:14.987134: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-06 22:26:15.015725: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-06 22:26:15.218609: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-06 22:26:16.301896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from skimage import exposure, filters\n",
    "from astropy.convolution import convolve, Gaussian2DKernel\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized image data array shape: (1683, 128, 128, 3)\n",
      "Encoded labels: ['cosmos space' 'cosmos space' 'cosmos space' ... 'stars' 'stars' 'stars']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Black Holes',\n",
       " 'constellation',\n",
       " 'cosmos space',\n",
       " 'galaxies',\n",
       " 'nebula',\n",
       " 'planets',\n",
       " 'stars'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Path to the FITS file\n",
    "fits_file_path = 'space images/output_images_with_labels.fits'\n",
    "\n",
    "# Lists to hold image data and labels\n",
    "image_data_list = []\n",
    "label_list = []\n",
    "\n",
    "# Desired image size\n",
    "# target_size = (256, 256)\n",
    "\n",
    "# Open the FITS file\n",
    "with fits.open(fits_file_path) as hdulist:\n",
    "    # Iterate over each ImageHDU (starting from index 1)\n",
    "    for hdu in hdulist[1:-1]:  # Skip the primary HDU (index 0) and the last HDU (label table)\n",
    "        if isinstance(hdu, fits.ImageHDU):\n",
    "            # Extract image data\n",
    "            image_data = hdu.data\n",
    "            \n",
    "            # Convert the image data to PIL Image for resizing\n",
    "            image = Image.fromarray(image_data)\n",
    "\n",
    "            # Resize the image to 256x256\n",
    "            # resized_image = image.resize(target_size)\n",
    "\n",
    "            # Convert the resized image back to NumPy array\n",
    "            resized_image_data = np.array(image)\n",
    "\n",
    "            # Append the resized image data and corresponding label to the lists\n",
    "            image_data_list.append(resized_image_data)\n",
    "            label_list.append(hdu.header.get('OBJECT'))\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "image_data_array = np.array(image_data_list)\n",
    "label_array = np.array(label_list)\n",
    "\n",
    "# Encode labels as integers\n",
    "# label_encoder = LabelEncoder()\n",
    "# encoded_labels = label_encoder.fit_transform(label_array)\n",
    "\n",
    "# Verify the shapes of the arrays\n",
    "print(f\"Resized image data array shape: {image_data_array.shape}\")\n",
    "# print(f\"Encoded labels: {encoded_labels}\")\n",
    "print(f\"Encoded labels: {label_array}\")\n",
    "set(label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `image_data_array` and `label_array` are already defined\n",
    "# Encode labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(label_array)\n",
    "\n",
    "# One-hot encode the labels for categorical cross-entropy loss\n",
    "one_hot_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# image_data_array = np.expand_dims(image_data_array, axis=-1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_data_array, one_hot_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 22:26:54.012510: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: UNKNOWN ERROR (100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load VGG16 model with pre-trained weights and exclude the top fully connected layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Add custom top layers\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# x = Dense(36, activation='relu')(x)\n",
    "predictions = Dense(len(np.unique(encoded_labels)), activation='softmax')(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardik/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 116ms/step - accuracy: 0.4124 - loss: 2.8088 - val_accuracy: 0.6142 - val_loss: 2.5182\n",
      "Epoch 2/25\n",
      "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 131ms/step - accuracy: 0.2500 - loss: 2.3413"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 22:27:42.565236: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.2500 - loss: 2.3413 - val_accuracy: 0.6172 - val_loss: 2.5073\n",
      "Epoch 3/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 116ms/step - accuracy: 0.5187 - loss: 1.6846 - val_accuracy: 0.6677 - val_loss: 2.2332\n",
      "Epoch 4/25\n",
      "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 99ms/step - accuracy: 0.2500 - loss: 2.3017"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 22:28:28.395276: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.2500 - loss: 2.3017 - val_accuracy: 0.6647 - val_loss: 2.2317\n",
      "Epoch 5/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 112ms/step - accuracy: 0.5761 - loss: 1.2601 - val_accuracy: 0.6736 - val_loss: 2.1044\n",
      "Epoch 6/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.2500 - loss: 1.2307 - val_accuracy: 0.6736 - val_loss: 2.1139\n",
      "Epoch 7/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 113ms/step - accuracy: 0.5888 - loss: 1.2152 - val_accuracy: 0.6795 - val_loss: 2.0935\n",
      "Epoch 8/25\n",
      "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 96ms/step - accuracy: 0.7500 - loss: 1.0134"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 22:30:33.532698: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.7500 - loss: 1.0134 - val_accuracy: 0.6766 - val_loss: 2.1108\n",
      "Epoch 9/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 125ms/step - accuracy: 0.5711 - loss: 1.1734 - val_accuracy: 0.6944 - val_loss: 2.1964\n",
      "Epoch 10/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.7500 - loss: 0.6346 - val_accuracy: 0.6973 - val_loss: 2.1955\n",
      "Epoch 11/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 117ms/step - accuracy: 0.5985 - loss: 1.1156 - val_accuracy: 0.7181 - val_loss: 1.8820\n",
      "Epoch 12/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.2500 - loss: 1.8566 - val_accuracy: 0.7122 - val_loss: 1.8787\n",
      "Epoch 13/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 116ms/step - accuracy: 0.6264 - loss: 1.0992 - val_accuracy: 0.6884 - val_loss: 2.1413\n",
      "Epoch 14/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.7500 - loss: 3.7667 - val_accuracy: 0.6973 - val_loss: 2.1358\n",
      "Epoch 15/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 106ms/step - accuracy: 0.6076 - loss: 1.0877 - val_accuracy: 0.6944 - val_loss: 2.2112\n",
      "Epoch 16/25\n",
      "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 87ms/step - accuracy: 0.7500 - loss: 0.8423"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 22:33:37.072173: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.7500 - loss: 0.8423 - val_accuracy: 0.6825 - val_loss: 2.2199\n",
      "Epoch 17/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 110ms/step - accuracy: 0.5441 - loss: 1.2278 - val_accuracy: 0.6736 - val_loss: 2.3350\n",
      "Epoch 18/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - loss: 2.2624 - val_accuracy: 0.6706 - val_loss: 2.3455\n",
      "Epoch 19/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 114ms/step - accuracy: 0.6095 - loss: 1.0948 - val_accuracy: 0.6706 - val_loss: 1.9441\n",
      "Epoch 20/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.5000 - loss: 2.2371 - val_accuracy: 0.6706 - val_loss: 1.9350\n",
      "Epoch 21/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 114ms/step - accuracy: 0.6247 - loss: 1.0408 - val_accuracy: 0.6736 - val_loss: 2.3247\n",
      "Epoch 22/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.7500 - loss: 0.4773 - val_accuracy: 0.6736 - val_loss: 2.3257\n",
      "Epoch 23/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 114ms/step - accuracy: 0.6519 - loss: 1.0163 - val_accuracy: 0.6973 - val_loss: 3.1147\n",
      "Epoch 24/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.7500 - loss: 0.7219 - val_accuracy: 0.6973 - val_loss: 3.0807\n",
      "Epoch 25/25\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 109ms/step - accuracy: 0.6318 - loss: 1.0693 - val_accuracy: 0.6884 - val_loss: 2.5149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fc2c4a2d510>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Prepare data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# # Define the callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "# model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Train the model with data augmentation and callbacks\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "          steps_per_epoch=len(X_train) // batch_size,\n",
    "          epochs=25,\n",
    "          validation_data=(X_test, y_test))\n",
    "        #   callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('Classifier.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 550ms/step - accuracy: 0.6778 - loss: 2.1629\n",
      "Test accuracy: 0.6884\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_object_type(img_path, model, label_encoder):\n",
    "    # Load and preprocess the image using PIL\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((128, 128))  # Resize to match the model input shape\n",
    "    img_array = np.array(img)  # Convert PIL image to numpy array\n",
    "    \n",
    "    # Ensure the image has 3 channels (RGB), convert if needed\n",
    "    if img_array.shape[-1] != 3:\n",
    "        img_array = np.stack([img_array] * 3, axis=-1)\n",
    "    \n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = preprocess_input(img_array)  # Preprocess the image as required by VGG16\n",
    "\n",
    "    # Predict the object type\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class_idx = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Convert the predicted index to the original label\n",
    "    predicted_label = label_encoder.inverse_transform(predicted_class_idx)\n",
    "    \n",
    "    print(f\"Predicted object: {predicted_label[0]}\")\n",
    "    return predicted_label[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predicted object: stars\n",
      "stars\n"
     ]
    }
   ],
   "source": [
    "model.load('Classifier.h5')\n",
    "# x=predict_object_type(r'space images/galaxies/12.jpg',model,label_encoder)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def enhance_color_bands(fits_files, object_type):\n",
    "    \"\"\"\n",
    "    Enhance and combine color bands into a single RGB image.\n",
    "\n",
    "    :param fits_files: List of paths to FITS files corresponding to different bands.\n",
    "    :param object_type: Type of astronomical object (e.g., 'black_hole', 'galaxy', 'nebula', 'star').\n",
    "    :return: RGB image array.\n",
    "    \"\"\"\n",
    "    bands = [fits.getdata(f) for f in fits_files]\n",
    "    if object_type == 'black_hole':\n",
    "        return enhance_black_hole_color(bands)\n",
    "    elif object_type == 'constellation':\n",
    "        return enhance_constellation_color(bands)\n",
    "    elif object_type == 'galaxy':\n",
    "        return enhance_galaxy_color(bands)\n",
    "    elif object_type == 'nebula':\n",
    "        return enhance_nebula_color(bands)\n",
    "    elif object_type == 'cosmos':\n",
    "        return enhance_cosmos_color(bands)\n",
    "    elif object_type == 'planet':\n",
    "        return enhance_planet_color(bands)\n",
    "    elif object_type == 'star':\n",
    "        return enhance_star_color(bands)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown object type\")\n",
    "\n",
    "def preprocess_band(band):\n",
    "    \"\"\"\n",
    "    Preprocess a single band: apply normalization and contrast stretching.\n",
    "\n",
    "    :param band: 2D array of the FITS band.\n",
    "    :return: Processed 2D array.\n",
    "    \"\"\"\n",
    "    band = exposure.rescale_intensity(band, in_range='image', out_range=(0, 1))\n",
    "    band = exposure.equalize_hist(band)\n",
    "    return band\n",
    "\n",
    "def enhance_black_hole_color(bands):\n",
    "    \"\"\"\n",
    "    Enhance and combine color bands for a black hole.\n",
    "    \"\"\"\n",
    "    red, green, blue = [preprocess_band(band) for band in bands]\n",
    "\n",
    "    # Combine bands into an RGB image\n",
    "    rgb_image = np.dstack([red, green, blue])\n",
    "    return rgb_image\n",
    "\n",
    "def enhance_constellation_color(bands):\n",
    "    \"\"\"\n",
    "    Enhance and combine color bands for star constellations.\n",
    "    \"\"\"\n",
    "    red, green, blue = [preprocess_band(band) for band in bands]\n",
    "    red = filters.unsharp_mask(red, radius=1, amount=2)\n",
    "    green = filters.unsharp_mask(green, radius=1, amount=2)\n",
    "    blue = filters.unsharp_mask(blue, radius=1, amount=2)\n",
    "    rgb_image = np.dstack([red, green, blue])\n",
    "    return rgb_image\n",
    "\n",
    "def enhance_galaxy_color(bands):\n",
    "    \"\"\"\n",
    "    Enhance and combine color bands for a galaxy.\n",
    "    \"\"\"\n",
    "    red, green, blue = [preprocess_band(band) for band in bands]\n",
    "    red = filters.unsharp_mask(red, radius=1, amount=1.5)\n",
    "    green = filters.unsharp_mask(green, radius=1, amount=1.5)\n",
    "    blue = filters.unsharp_mask(blue, radius=1, amount=1.5)\n",
    "    rgb_image = np.dstack([red, green, blue])\n",
    "    return rgb_image\n",
    "\n",
    "def enhance_nebula_color(bands):\n",
    "    \"\"\"\n",
    "    Enhance and combine color bands for a nebula.\n",
    "    \"\"\"\n",
    "    red, green, blue = [preprocess_band(band) for band in bands]\n",
    "    red = filters.unsharp_mask(red, radius=1, amount=2)\n",
    "    green = filters.unsharp_mask(green, radius=1, amount=2)\n",
    "    blue = filters.unsharp_mask(blue, radius=1, amount=2)\n",
    "    rgb_image = np.dstack([red, green, blue])\n",
    "    return rgb_image\n",
    "\n",
    "def enhance_cosmos_color(bands):\n",
    "    \"\"\"\n",
    "    Enhance and combine color bands for a wide-field cosmos image.\n",
    "    \"\"\"\n",
    "    red, green, blue = [preprocess_band(band) for band in bands]\n",
    "    kernel = Gaussian2DKernel(x_stddev=1)\n",
    "    red_smooth = convolve(red, kernel)\n",
    "    green_smooth = convolve(green, kernel)\n",
    "    blue_smooth = convolve(blue, kernel)\n",
    "    rgb_image = np.dstack([red_smooth, green_smooth, blue_smooth])\n",
    "    return rgb_image\n",
    "\n",
    "def enhance_planet_color(bands):\n",
    "    \"\"\"\n",
    "    Enhance and combine color bands for a planet.\n",
    "    \"\"\"\n",
    "    red, green, blue = [preprocess_band(band) for band in bands]\n",
    "    red = filters.unsharp_mask(red, radius=1, amount=1.5)\n",
    "    green = filters.unsharp_mask(green, radius=1, amount=1.5)\n",
    "    blue = filters.unsharp_mask(blue, radius=1, amount=1.5)\n",
    "    rgb_image = np.dstack([red, green, blue])\n",
    "    return rgb_image\n",
    "\n",
    "def enhance_star_color(bands):\n",
    "    \"\"\"\n",
    "    Enhance and combine color bands for star fields.\n",
    "    \"\"\"\n",
    "    red, green, blue = [preprocess_band(band) for band in bands]\n",
    "    red_filtered = filters.unsharp_mask(red, radius=1, amount=2)\n",
    "    green_filtered = filters.unsharp_mask(green, radius=1, amount=2)\n",
    "    blue_filtered = filters.unsharp_mask(blue, radius=1, amount=2)\n",
    "    rgb_image = np.dstack([red_filtered, green_filtered, blue_filtered])\n",
    "    return rgb_image\n",
    "\n",
    "def display_image(image, title=\"Enhanced Image\"):\n",
    "    \"\"\"\n",
    "    Display an RGB image using matplotlib.\n",
    "\n",
    "    :param image: 3D array of the RGB image.\n",
    "    :param title: Title of the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, origin='lower')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def output_final(fits_file,model,label_encoder):\n",
    "    \"\"\"\n",
    "    Split a single FITS file into three-band FITS files: Red, Green, and Blue.\n",
    "\n",
    "    :param fits_file: Path to the single-band FITS file.\n",
    "    :return: List of paths to the new Red, Green, and Blue FITS files.\n",
    "    \"\"\"\n",
    "    # Load the single-band FITS file\n",
    "    with fits.open(fits_file) as hdul:\n",
    "        data = hdul[0].data\n",
    "    \n",
    "    # Check the data shape and type\n",
    "    if data.ndim != 2:\n",
    "        raise ValueError(\"The FITS file must contain a 2D array.\")\n",
    "    \n",
    "    # Create three bands (red, green, blue) from the original data\n",
    "    # Here, we're simply duplicating the original data for all three bands\n",
    "    # In a real scenario, you might want to use different data or apply transformations\n",
    "    red_band = data\n",
    "    green_band = data\n",
    "    blue_band = data\n",
    "    \n",
    "    # Define paths for the new FITS files\n",
    "    red_path = fits_file.replace('.fits', '_red.fits')\n",
    "    green_path = fits_file.replace('.fits', '_green.fits')\n",
    "    blue_path = fits_file.replace('.fits', '_blue.fits')\n",
    "\n",
    "    # Save the bands as separate FITS files\n",
    "    fits.writeto(red_path, red_band, overwrite=True)\n",
    "    fits.writeto(green_path, green_band, overwrite=True)\n",
    "    fits.writeto(blue_path, blue_band, overwrite=True)\n",
    "\n",
    "    if data.ndim != 2:\n",
    "        raise ValueError(\"The FITS file must contain a 2D array.\")\n",
    "    \n",
    "    # Normalize data to [0, 1]\n",
    "    data_min = np.min(data)\n",
    "    data_max = np.max(data)\n",
    "    normalized_data = (data - data_min) / (data_max - data_min)\n",
    "    \n",
    "    # Convert normalized data to [0, 255] for saving as image\n",
    "    image_data = (normalized_data * 255).astype(np.uint8)\n",
    "\n",
    "    object=predict_object_type(image_data,model,label_encoder)\n",
    "    fits_files=[red_path,green_path,blue_path]\n",
    "    enhanced_image=enhance_color_bands(fits_files,object)\n",
    "    display_image(enhanced_image, title=f\"Enhanced {object.replace('_', ' ').capitalize()} Image\")\n",
    "\n",
    "    return enhanced_image,object\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
